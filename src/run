#!/usr/bin/env python
# -*- coding: utf-8 -*-

from datetime import datetime
import json
import logging
from os import getenv
import subprocess

from logic import (
    exists_and_not_empty,
    generate_dates,
    get_filename,
)

logger = logging.getLogger(__name__)


CACHEDIR = getenv('CACHEDIR')
OUTDIR = getenv('OUTDIR')
MODES = json.loads(getenv('MODES'))
REPROCESS_REPORTS = getenv('REPROCESS_REPORTS') == 'true'
XREF_START_DATE = getenv('XREF_START_DATE')


def run():
    cutoff_date = datetime.now().strftime('%Y-%m-%d')
    earliest_date = XREF_START_DATE

    # cache Crossref cited-by reports
    for day in generate_dates(earliest_date, cutoff_date):
        date = day.strftime('%Y-%m-%d')
        out_file = get_filename(CACHEDIR, date, extension='xml')

        try:
            assert not exists_and_not_empty(out_file)
        except AssertionError:
            continue

        cmd = f'./retrieve_cited_by_stats --date {date} --out_file {out_file}'
        subprocess.call(cmd.split())

    # normalise reports
    for day in generate_dates(earliest_date, cutoff_date):

        date = day.strftime('%Y-%m-%d')
        out_file = get_filename(OUTDIR, date, extension='csv')

        # continue if output file already exists
        if exists_and_not_empty(out_file) and not REPROCESS_REPORTS:
            continue

        cache_file = get_filename(CACHEDIR, date, extension='xml')
        # at this point all *relevant* cache files must exists
        if not exists_and_not_empty(cache_file):
            continue

        for i, m in enumerate(MODES):
            # only include headers in first iteration
            cmd = (
                f'./resolve_reports --measure {m["measure"]}'
                f' --in_file {cache_file} --out_file {out_file}'
            ).split()

            add_headers = ['--add-headers'] if i == 0 else []
            cmd.extend(add_headers)

            subprocess.call(cmd)


if __name__ == '__main__':
    run()
